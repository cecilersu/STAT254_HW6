{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-challenge",
   "metadata": {},
   "source": [
    "# The Annotated DeepDream\n",
    "\n",
    "The idea of this notebook is to make it easy to get started with the [DeepDream algorithm](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)!\n",
    "\n",
    "## So what is DeepDream in a nutshell?\n",
    "\n",
    "DeepDream is an *artistic algorithm* where a **pretrained [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)** is fed an image and *optimized* to **amplify the features** it \"sees\" in the image.\n",
    "\n",
    "We do that by doing a **[gradient ascent](https://en.wikipedia.org/wiki/Gradient_descent)** on the (usually) **[MSE loss](https://en.wikipedia.org/wiki/Mean_squared_error)** constructed over the activations coming from a certain layer. The MSE is constructed between those activations and the all-0s tensor, which would by itself push the highest activations strongest towards zero, but by switching the sign, and doing a gradient ascent instead of descent, we end up amplyfing them the most!\n",
    "\n",
    "Depending on the neural network layer the features amplified will either be **low level** (like edges, certain geometric patterns, etc.) or **high level** (like dog snouts, eyes, etc.) that heavily **depends on the dataset** on which the net was pretrained!\n",
    "\n",
    "With that you're ready to start digging into the actual code! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-talent",
   "metadata": {},
   "source": [
    "# Part 1: Developing the DeepDream algorithm\n",
    "\n",
    "In this notebook I'll be focusing on a single CNN: the **VGG16** (and I'll briefly touch upon ResNet50). You can see how the net architecture looks like below:\n",
    "\n",
    "<img src=\"data/examples/jupyter/vgg16.png\" alt=\"VGG 16 architecture\" align=\"center\" style=\"width: 700px;\"/> <br/>\n",
    "\n",
    "You have other awesome models at your disposal in this repository like: **ResNet50**, **GoogLeNet**, and **AlexNet**.\n",
    "\n",
    "But I'll focus on a single model and hopefully you'll understand the main concepts! I'll also teach you how to experiment with this repo to create whatever you want.\n",
    "\n",
    "Also as I mentioned, the dataset on which the net was trained really matters, and aside from **ImageNet** pretrained weights (which is fairly standard), ResNet50 and AlexNet have support for **[Places365](http://places2.csail.mit.edu/)** pretrained weights. You'll soon see how I did it and you can add support for other datasets! If you decide to do that and you get some beautiful, but different results, I strongly encourage you to [open up a PR](https://github.com/gordicaleksa/pytorch-deepdream/pulls)! (I just beg you to adhere to the coding style of this repo) ‚ù§Ô∏è\n",
    "\n",
    "Ok, enough talk! Let's dig into the code and first define some imports that we'll need: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "based-northern",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T23:10:49.389871Z",
     "start_time": "2023-12-10T23:10:46.621213Z"
    }
   },
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (but we don't have those here)\n",
    "\n",
    "# Python native libs\n",
    "import os\n",
    "import enum\n",
    "from collections import namedtuple\n",
    "import argparse\n",
    "import numbers\n",
    "import math\n",
    "\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-turtle",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Now define some enums and constants that will come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cleared-winter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T23:10:49.390811Z",
     "start_time": "2023-12-10T23:10:48.916449Z"
    }
   },
   "outputs": [],
   "source": [
    "# The 2 datasets we'll be leveraging\n",
    "class SupportedPretrainedWeights(enum.Enum):\n",
    "    IMAGENET = 0\n",
    "    PLACES_365 = 1\n",
    "    \n",
    "\n",
    "# The 2 models we'll be using\n",
    "class SupportedModels(enum.Enum):\n",
    "    VGG16_EXPERIMENTAL = 0,\n",
    "    RESNET50 = 1\n",
    "    \n",
    "\n",
    "# Commonly used paths, let's define them here as constants\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "INPUT_DATA_PATH = os.path.join(DATA_DIR_PATH, 'input')\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')\n",
    "OUT_IMAGES_PATH = os.path.join(DATA_DIR_PATH, 'out-images')\n",
    "\n",
    "# Make sure these exist as the rest of the code relies on it\n",
    "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "os.makedirs(OUT_IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU\n",
    "\n",
    "# Images will be normalized using these, because the CNNs were trained with normalized images as well!\n",
    "IMAGENET_MEAN_1 = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD_1 = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-newton",
   "metadata": {},
   "source": [
    "With that out of the way we've got the level 1 unlocked (exposing model activations ü¶Ñ). üòç Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-proposition",
   "metadata": {},
   "source": [
    "## Exposing neural network's activations \n",
    "Nice! Now let's expose the activations coming from the VGG16 as we'll need those later and you'll soon understand exactly why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_weights_path=None):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "        # If a path to pretrained weights is provided, load the weights\n",
    "        if pretrained_weights_path is not None:\n",
    "            self.load_pretrained_weights(pretrained_weights_path)\n",
    "\n",
    "        # Expose layers for visualization\n",
    "        self.layer_names = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'max_pooling1', \n",
    "                            'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'max_pooling2']\n",
    "        self.activations = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        self.activations['conv1_1'] = x\n",
    "        x = self.activations['relu1_1'] = self.conv1[1](x)\n",
    "        x = self.activations['conv1_2'] = self.conv1[2](x)\n",
    "        x = self.activations['relu1_2'] = self.conv1[3](x)\n",
    "        x = self.activations['max_pooling1'] = self.conv1[4](x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        self.activations['conv2_1'] = x\n",
    "        x = self.activations['relu2_1'] = self.conv2[1](x)\n",
    "        x = self.activations['conv2_2'] = self.conv2[2](x)\n",
    "        x = self.activations['relu2_2'] = self.conv2[3](x)\n",
    "        x = self.activations['max_pooling2'] = self.conv2[4](x)\n",
    "\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization\n",
    "\n",
    "    def load_pretrained_weights(self, pretrained_weights_path):\n",
    "        if not os.path.exists(pretrained_weights_path):\n",
    "            raise FileNotFoundError(f\"Pretrained weights file not found at {pretrained_weights_path}\")\n",
    "\n",
    "        state_dict = torch.load(pretrained_weights_path)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "# Example usage:\n",
    "# model = CNN()\n",
    "# output, intermediate_activations = model(torch.randn(1, 1, 28, 28))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T23:10:53.792654Z",
     "start_time": "2023-12-10T23:10:53.710304Z"
    }
   },
   "id": "723b8ef0cfd675ee"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def fetch_and_prepare_model(model_type, pretrained_weights):\n",
    "    if model_type == SupportedModels.VGG16_EXPERIMENTAL.name:\n",
    "        model = Vgg16Experimental(pretrained_weights, requires_grad=False, show_progress=True).to(DEVICE)\n",
    "    elif model_type == SupportedModels.RESNET50.name:\n",
    "        # We'll define the ResNet50 later\n",
    "        model = ResNet50(pretrained_weights, requires_grad=False, show_progress=True).to(DEVICE)\n",
    "    elif model_type == 'MNIST_CECILE':\n",
    "        model = CNN(pretrained_weights_path=pretrained_weights).to(DEVICE)\n",
    "    else:\n",
    "        raise Exception('Model not yet supported.')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T23:10:56.916509Z",
     "start_time": "2023-12-10T23:10:56.017699Z"
    }
   },
   "id": "a313be6ff67f2cc2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cubic-culture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T23:10:57.920464Z",
     "start_time": "2023-12-10T23:10:57.851196Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vgg16Experimental(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_weights, requires_grad=False, show_progress=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Only ImageNet weights are supported for now for this model\n",
    "        if pretrained_weights == SupportedPretrainedWeights.IMAGENET.name:\n",
    "            vgg16 = models.vgg16(pretrained=True, progress=show_progress).eval()\n",
    "        else:\n",
    "            raise Exception(f'Pretrained weights {pretrained_weights} not yet supported for {self.__class__.__name__} model.')\n",
    "\n",
    "        # I just used the official PyTorch implementation to figure out how to dissect VGG16:\n",
    "        # https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "        vgg_pretrained_features = vgg16.features\n",
    "\n",
    "        # I've exposed the best/most interesting layers in my subjective opinion (mp5 is not that good though)\n",
    "        self.layer_names = ['relu3_1', 'relu3_2', 'relu3_3', 'relu4_1', 'relu4_2', 'relu4_3', 'relu5_1', 'relu5_2', 'relu5_3', 'mp5']\n",
    "\n",
    "        # 31 layers in total for the VGG16\n",
    "        self.conv1_1 = vgg_pretrained_features[0]\n",
    "        self.relu1_1 = vgg_pretrained_features[1]\n",
    "        self.conv1_2 = vgg_pretrained_features[2]\n",
    "        self.relu1_2 = vgg_pretrained_features[3]\n",
    "        self.max_pooling1 = vgg_pretrained_features[4]\n",
    "        self.conv2_1 = vgg_pretrained_features[5]\n",
    "        self.relu2_1 = vgg_pretrained_features[6]\n",
    "        self.conv2_2 = vgg_pretrained_features[7]\n",
    "        self.relu2_2 = vgg_pretrained_features[8]\n",
    "        self.max_pooling2 = vgg_pretrained_features[9]\n",
    "        self.conv3_1 = vgg_pretrained_features[10]\n",
    "        self.relu3_1 = vgg_pretrained_features[11]\n",
    "        self.conv3_2 = vgg_pretrained_features[12]\n",
    "        self.relu3_2 = vgg_pretrained_features[13]\n",
    "        self.conv3_3 = vgg_pretrained_features[14]\n",
    "        self.relu3_3 = vgg_pretrained_features[15]\n",
    "        self.max_pooling3 = vgg_pretrained_features[16]\n",
    "        self.conv4_1 = vgg_pretrained_features[17]\n",
    "        self.relu4_1 = vgg_pretrained_features[18]\n",
    "        self.conv4_2 = vgg_pretrained_features[19]\n",
    "        self.relu4_2 = vgg_pretrained_features[20]\n",
    "        self.conv4_3 = vgg_pretrained_features[21]\n",
    "        self.relu4_3 = vgg_pretrained_features[22]\n",
    "        self.max_pooling4 = vgg_pretrained_features[23]\n",
    "        self.conv5_1 = vgg_pretrained_features[24]\n",
    "        self.relu5_1 = vgg_pretrained_features[25]\n",
    "        self.conv5_2 = vgg_pretrained_features[26]\n",
    "        self.relu5_2 = vgg_pretrained_features[27]\n",
    "        self.conv5_3 = vgg_pretrained_features[28]\n",
    "        self.relu5_3 = vgg_pretrained_features[29]\n",
    "        self.max_pooling5 = vgg_pretrained_features[30]\n",
    "\n",
    "        # Turn off these because we'll be using a pretrained network\n",
    "        # if we didn't do this PyTorch would be saving gradients and eating up precious memory!\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Just expose every single layer during the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_1(x)\n",
    "        conv1_1 = x\n",
    "        x = self.relu1_1(x)\n",
    "        relu1_1 = x\n",
    "        x = self.conv1_2(x)\n",
    "        conv1_2 = x\n",
    "        x = self.relu1_2(x)\n",
    "        relu1_2 = x\n",
    "        x = self.max_pooling1(x)\n",
    "        x = self.conv2_1(x)\n",
    "        conv2_1 = x\n",
    "        x = self.relu2_1(x)\n",
    "        relu2_1 = x\n",
    "        x = self.conv2_2(x)\n",
    "        conv2_2 = x\n",
    "        x = self.relu2_2(x)\n",
    "        relu2_2 = x\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.conv3_1(x)\n",
    "        conv3_1 = x\n",
    "        x = self.relu3_1(x)\n",
    "        relu3_1 = x\n",
    "        x = self.conv3_2(x)\n",
    "        conv3_2 = x\n",
    "        x = self.relu3_2(x)\n",
    "        relu3_2 = x\n",
    "        x = self.conv3_3(x)\n",
    "        conv3_3 = x\n",
    "        x = self.relu3_3(x)\n",
    "        relu3_3 = x\n",
    "        x = self.max_pooling3(x)\n",
    "        x = self.conv4_1(x)\n",
    "        conv4_1 = x\n",
    "        x = self.relu4_1(x)\n",
    "        relu4_1 = x\n",
    "        x = self.conv4_2(x)\n",
    "        conv4_2 = x\n",
    "        x = self.relu4_2(x)\n",
    "        relu4_2 = x\n",
    "        x = self.conv4_3(x)\n",
    "        conv4_3 = x\n",
    "        x = self.relu4_3(x)\n",
    "        relu4_3 = x\n",
    "        x = self.max_pooling4(x)\n",
    "        x = self.conv5_1(x)\n",
    "        conv5_1 = x\n",
    "        x = self.relu5_1(x)\n",
    "        relu5_1 = x\n",
    "        x = self.conv5_2(x)\n",
    "        conv5_2 = x\n",
    "        x = self.relu5_2(x)\n",
    "        relu5_2 = x\n",
    "        x = self.conv5_3(x)\n",
    "        conv5_3 = x\n",
    "        x = self.relu5_3(x)\n",
    "        relu5_3 = x\n",
    "        mp5 = self.max_pooling5(x)\n",
    "\n",
    "        # Finally, expose only the layers that you want to experiment with here\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
    "        out = vgg_outputs(relu3_1, relu3_2, relu3_3, relu4_1, relu4_2, relu4_3, relu5_1, relu5_2, relu5_3, mp5)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def fetch_and_prepare_model(model_type, pretrained_weights):\n",
    "    if model_type == SupportedModels.VGG16_EXPERIMENTAL.name:\n",
    "        model = Vgg16Experimental(pretrained_weights, requires_grad=False, show_progress=True).to(DEVICE)\n",
    "    elif model_type == SupportedModels.RESNET50.name:\n",
    "        # We'll define the ResNet50 later\n",
    "        model = ResNet50(pretrained_weights, requires_grad=False, show_progress=True).to(DEVICE)\n",
    "    else:\n",
    "        raise Exception('Model not yet supported.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-retreat",
   "metadata": {},
   "source": [
    "Nice as you can see VGG16 is just a bunch of **convolutions** and **ReLUs** followed by occasional **max pooling** layers.\n",
    "Doesn't get much simpler than this!\n",
    "\n",
    "I'll assume in this notebook that you're familiar with what these are, but even if you're not you can just treat them as a black box. ‚¨õ\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ exposing model activations\n",
    "\n",
    "## Image loading, saving and displaying üñºÔ∏è\n",
    "\n",
    "\n",
    "\n",
    "Let's now define some image utility functions that will handle data (images) loading, saving and plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "western-communications",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T23:11:14.932943Z",
     "start_time": "2023-12-10T23:11:13.868653Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, target_shape=None):\n",
    "    if not os.path.exists(img_path):\n",
    "        raise Exception(f'Path does not exist: {img_path}')\n",
    "    img = cv.imread(img_path)[:, :, ::-1]  # [:, :, ::-1] converts BGR (opencv format...) into RGB\n",
    "\n",
    "    if target_shape is not None:  # resize section\n",
    "        if isinstance(target_shape, int) and target_shape != -1:  # scalar -> implicitly setting the width\n",
    "            current_height, current_width = img.shape[:2]\n",
    "            new_width = target_shape\n",
    "            new_height = int(current_height * (new_width / current_width))\n",
    "            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n",
    "        else:  # set both dimensions to target shape\n",
    "            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n",
    "\n",
    "    # This need to go after resizing - otherwise cv.resize will push values outside of [0,1] range\n",
    "    img = img.astype(np.float32)  # convert from uint8 to float32\n",
    "    img /= 255.0  # get to [0, 1] range\n",
    "    return img\n",
    "\n",
    "\n",
    "# config is just a shared dictionary that you'll be seeing used everywhere, but we'll define it a bit later.\n",
    "# For the time being think of it as an oracle - whatever the function needs - config provides ^^\n",
    "def save_and_maybe_display_image(config, dump_img, name_modifier=None):\n",
    "    assert isinstance(dump_img, np.ndarray), f'Expected numpy array got {type(dump_img)}.'\n",
    "\n",
    "    # Step 1: figure out the dump dir location\n",
    "    dump_dir = config['dump_dir']\n",
    "    os.makedirs(dump_dir, exist_ok=True)\n",
    "\n",
    "    # Step 2: define the output image name\n",
    "    if name_modifier is not None:\n",
    "        dump_img_name = str(name_modifier).zfill(6) + '.jpg'\n",
    "    else:\n",
    "        dump_img_name = build_image_name(config)\n",
    "\n",
    "    if dump_img.dtype != np.uint8:\n",
    "        dump_img = (dump_img*255).astype(np.uint8)\n",
    "\n",
    "    # Step 3: write image to the file system\n",
    "    # ::-1 because opencv expects BGR (and not RGB) format...\n",
    "    dump_path = os.path.join(dump_dir, dump_img_name)\n",
    "    cv.imwrite(dump_path, dump_img[:, :, ::-1])\n",
    "\n",
    "    # Step 4: potentially display/plot the image\n",
    "    if config['should_display']:\n",
    "        fig = plt.figure(figsize=(7.5,5), dpi=100)  # otherwise plots are really small in Jupyter Notebook\n",
    "        plt.imshow(dump_img)\n",
    "        plt.show()\n",
    "\n",
    "    return dump_path\n",
    "\n",
    "\n",
    "# This function makes sure we can later reconstruct the image using the information encoded into the filename!\n",
    "# Again don't worry about all the arguments we'll define them later\n",
    "def build_image_name(config):\n",
    "    input_name = 'rand_noise' if config['use_noise'] else config['input'].split('.')[0]\n",
    "    layers = '_'.join(config['layers_to_use'])\n",
    "    # Looks awful but makes the creation process transparent for other creators\n",
    "    img_name = f'{input_name}_width_{config[\"img_width\"]}_model_{config[\"model_name\"]}_{config[\"pretrained_weights\"]}_{layers}_pyrsize_{config[\"pyramid_size\"]}_pyrratio_{config[\"pyramid_ratio\"]}_iter_{config[\"num_gradient_ascent_iterations\"]}_lr_{config[\"lr\"]}_shift_{config[\"spatial_shift_size\"]}_smooth_{config[\"smoothing_coefficient\"]}.jpg'\n",
    "    return img_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-manner",
   "metadata": {},
   "source": [
    "Awesome! Let's test the code as we progress, to make sure it works the way we expect it to (without any üêõüêõüêõ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "identified-semester",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T23:11:21.690440Z",
     "start_time": "2023-12-10T23:11:20.264906Z"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Path does not exist: /Users/cecilerousseau/Documents/BERKELEY/STAT 254 001 - STATS & ML/hw6_jupyter_notebook/data/input/campanile.jpeg",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m img_width \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m  \u001B[38;5;66;03m# arbitrary\u001B[39;00m\n\u001B[1;32m      3\u001B[0m img_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(INPUT_DATA_PATH, input_img_name)\n\u001B[0;32m----> 4\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mload_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg_width\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m fig \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m7.5\u001B[39m,\u001B[38;5;241m5\u001B[39m), dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)  \u001B[38;5;66;03m# otherwise plots are really small in Jupyter Notebook\u001B[39;00m\n\u001B[1;32m      7\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(img)\n",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m, in \u001B[0;36mload_image\u001B[0;34m(img_path, target_shape)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_image\u001B[39m(img_path, target_shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(img_path):\n\u001B[0;32m----> 3\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPath does not exist: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m     img \u001B[38;5;241m=\u001B[39m cv\u001B[38;5;241m.\u001B[39mimread(img_path)[:, :, ::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]  \u001B[38;5;66;03m# [:, :, ::-1] converts BGR (opencv format...) into RGB\u001B[39;00m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m target_shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# resize section\u001B[39;00m\n",
      "\u001B[0;31mException\u001B[0m: Path does not exist: /Users/cecilerousseau/Documents/BERKELEY/STAT 254 001 - STATS & ML/hw6_jupyter_notebook/data/input/campanile.jpeg"
     ]
    }
   ],
   "source": [
    "input_img_name = 'campanile.jpeg'  # checked in, I'll be using it as the running example\n",
    "img_width = 500  # arbitrary\n",
    "img_path = os.path.join(INPUT_DATA_PATH, input_img_name)\n",
    "img = load_image(img_path, target_shape=img_width)\n",
    "\n",
    "fig = plt.figure(figsize=(7.5,5), dpi=100)  # otherwise plots are really small in Jupyter Notebook\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-preserve",
   "metadata": {},
   "source": [
    "‚úÖ Image loading works like a charm! \n",
    "\n",
    "We'll test `save_and_maybe_display_image` a bit later (we need that config!).\n",
    "\n",
    "## DeepDream image/tensor utilities\n",
    "\n",
    "Now before jumping straight into the core of the algorithm let's define a couple more functions that we'll need, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "surface-swiss",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.643891Z",
     "start_time": "2023-12-10T22:52:34.014820Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean/std normalization - ImageNet's mean and std capture the statistics of natural images pretty nicely.\n",
    "# This works for Places365 dataset as well (keep in mind you might have to change it if your dataset is way different)\n",
    "\n",
    "def pre_process_numpy_img(img):\n",
    "    assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "    img = (img - IMAGENET_MEAN_1) / IMAGENET_STD_1  # normalize image\n",
    "    return img\n",
    "\n",
    "\n",
    "def post_process_numpy_img(img):\n",
    "    assert isinstance(img, np.ndarray), f'Expected numpy image got {type(img)}'\n",
    "\n",
    "    if img.shape[0] == 3:  # if channel-first format move to channel-last (CHW -> HWC)\n",
    "        img = np.moveaxis(img, 0, 2)\n",
    "\n",
    "    mean = IMAGENET_MEAN_1.reshape(1, 1, -1)\n",
    "    std = IMAGENET_STD_1.reshape(1, 1, -1)\n",
    "    img = (img * std) + mean  # de-normalize\n",
    "    img = np.clip(img, 0., 1.)  # make sure it's in the [0, 1] range\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def pytorch_input_adapter(img):\n",
    "    # shape = (1, 3, H, W)\n",
    "    tensor = transforms.ToTensor()(img).to(DEVICE).unsqueeze(0)\n",
    "    tensor.requires_grad = True  # we need to collect gradients for the input image\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pytorch_output_adapter(tensor):\n",
    "    # Push to CPU, detach from the computational graph, convert from (1, 3, H, W) tensor into (H, W, 3) numpy image\n",
    "    return np.moveaxis(tensor.to('cpu').detach().numpy()[0], 0, 2)\n",
    "\n",
    "\n",
    "# Adds stochasticity to the algorithm and makes the results more diverse\n",
    "def random_circular_spatial_shift(tensor, h_shift, w_shift, should_undo=False):\n",
    "    if should_undo:\n",
    "        h_shift = -h_shift\n",
    "        w_shift = -w_shift\n",
    "    with torch.no_grad():\n",
    "        rolled = torch.roll(tensor, shifts=(h_shift, w_shift), dims=(2, 3))\n",
    "        rolled.requires_grad = True\n",
    "        return rolled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-passenger",
   "metadata": {},
   "source": [
    "With those out of the way, let me now tell you a bit more about pyramids. Pyramids were build in Egypt for ... wait what? üòÆ\n",
    "\n",
    "That's not the pyramid I'm talking about, let's see what I mean by **pyramid**! (if you're not already familiar with the concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-theta",
   "metadata": {},
   "source": [
    "## Image pyramid\n",
    "\n",
    "I mentioned in my initial explanation of DeepDream that the image will be fed through the CNN and that it will amplify *certain* features.\n",
    "\n",
    "There is one important, practical detail that I've omitted and that's that we'll be feeding the image in various resolutions into the CNN. By doing that the network will see different things each time and that will in return give us a richer output.\n",
    "\n",
    "**Note:** (for nerds ü§ì) *that happens because the ratio between the **receptive field** of the CNN and the input image changes and thus sometimes the net will see the entire image and thus can create global features and in other cases it will see a small portion of the image and it can focus more on the texture.*\n",
    "\n",
    "In order to feed multiple resolutions in, we'll need to define something called an **image pyramid**.\n",
    "\n",
    "Let's see what an image pyramid is (it'll be easier to explain that way):\n",
    "\n",
    "<img src=\"data/examples/jupyter/image_pyramid.png\" alt=\"Image pyramid\" align=\"center\" style=\"width: 350px;\"/> <br/>\n",
    "\n",
    "Awesome. Now what you can see here is the following, the input image like `figures.jpg` that we've been using will be **resized to multiple resolutions** and passed through the DeepDream algorithm. Depending on the resolution the neural network will **amplify different features**. It's a very common technique used all over *digital image processing* üì∑ and *classical computer vision* üíª worlds (pre-deep learning era kids! ü§ì).\n",
    "\n",
    "---\n",
    "\n",
    "Let's define a helper function that will calculate the image resolution depending on the pyramid level we're at.\n",
    "\n",
    "* `pyramid_size` - controls the number of pyramid levels (**5** in the image above)\n",
    "* `pyramid_ratio` - ratio of the resolutions in the pyramid. 2x means 400x400 image becomes 200x200 on the next level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "powered-phase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.645236Z",
     "start_time": "2023-12-10T22:52:34.023644Z"
    }
   },
   "outputs": [],
   "source": [
    "# again treat config as an oracle\n",
    "def get_new_shape(config, original_shape, current_pyramid_level):\n",
    "    SHAPE_MARGIN = 10\n",
    "    pyramid_ratio = config['pyramid_ratio']\n",
    "    pyramid_size = config['pyramid_size']\n",
    "    exponent = current_pyramid_level - pyramid_size + 1  # this one will be negative so we're dividing the original img shape\n",
    "    new_shape = np.round(np.float32(original_shape) * (pyramid_ratio**exponent)).astype(np.int32)\n",
    "\n",
    "    if new_shape[0] < SHAPE_MARGIN or new_shape[1] < SHAPE_MARGIN:\n",
    "        print(f'Pyramid size {config[\"pyramid_size\"]} with pyramid ratio {config[\"pyramid_ratio\"]} gives too small pyramid levels with size={new_shape}')\n",
    "        print(f'Please change the parameters.')\n",
    "        exit(0)\n",
    "\n",
    "    return new_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-kingston",
   "metadata": {},
   "source": [
    "We're now ready to dive into the core of the algorithm!!! üéâüéâüéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-quality",
   "metadata": {},
   "source": [
    "## DeepDream core algorithm\n",
    "\n",
    "So this is the core part. Take some time to understand what is happening. We'll define `gradient_ascent` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "former-frame",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.645621Z",
     "start_time": "2023-12-10T22:52:34.034268Z"
    }
   },
   "outputs": [],
   "source": [
    "def deep_dream_static_image(config, img=None):\n",
    "    model = fetch_and_prepare_model(config['model_name'], config['pretrained_weights'])\n",
    "\n",
    "    try:\n",
    "        layer_ids_to_use = [model.layer_names.index(layer_name) for layer_name in config['layers_to_use']]\n",
    "    except Exception as e:  # making sure you set the correct layer name for this specific model\n",
    "        print(f'Invalid layer names {[layer_name for layer_name in config[\"layers_to_use\"]]}.')\n",
    "        print(f'Available layers for model {config[\"model_name\"]} are {model.layer_names}.')\n",
    "        return\n",
    "\n",
    "    if img is None:  # load either the provided image or start from a pure noise image\n",
    "        img_path = os.path.join(INPUT_DATA_PATH, config['input'])\n",
    "        # load a numpy, [0, 1] range, channel-last, RGB image\n",
    "        img = load_image(img_path, target_shape=config['img_width'])\n",
    "        if config['use_noise']:\n",
    "            shape = img.shape\n",
    "            img = np.random.uniform(low=0.0, high=1.0, size=shape).astype(np.float32)\n",
    "\n",
    "    img = pre_process_numpy_img(img)\n",
    "    original_shape = img.shape[:-1]  # save initial height and width\n",
    "\n",
    "    # Note: simply rescaling the whole result (and not only details, see original implementation) gave me better results\n",
    "    # Going from smaller to bigger resolution (from pyramid top to bottom)\n",
    "    for pyramid_level in range(config['pyramid_size']):\n",
    "        new_shape = get_new_shape(config, original_shape, pyramid_level)\n",
    "        img = cv.resize(img, (new_shape[1], new_shape[0]))  # resize depending on the current pyramid level\n",
    "        input_tensor = pytorch_input_adapter(img)  # convert to trainable tensor\n",
    "\n",
    "        for iteration in range(config['num_gradient_ascent_iterations']):\n",
    "            \n",
    "            # Introduce some randomness, it will give us more diverse results especially when you're making videos\n",
    "            h_shift, w_shift = np.random.randint(-config['spatial_shift_size'], config['spatial_shift_size'] + 1, 2)\n",
    "            input_tensor = random_circular_spatial_shift(input_tensor, h_shift, w_shift)\n",
    "\n",
    "            # This is where the magic happens, treat it as a black box until the next cell\n",
    "            gradient_ascent(config, model, input_tensor, layer_ids_to_use, iteration)\n",
    "\n",
    "            # Roll back by the same amount as above (hence should_undo=True)\n",
    "            input_tensor = random_circular_spatial_shift(input_tensor, h_shift, w_shift, should_undo=True)\n",
    "\n",
    "        img = pytorch_output_adapter(input_tensor)\n",
    "\n",
    "    return post_process_numpy_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-annotation",
   "metadata": {},
   "source": [
    "And finally this is where the magic happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustained-melbourne",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.645933Z",
     "start_time": "2023-12-10T22:52:34.037910Z"
    }
   },
   "outputs": [],
   "source": [
    "LOWER_IMAGE_BOUND = torch.tensor((-IMAGENET_MEAN_1 / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "UPPER_IMAGE_BOUND = torch.tensor(((1 - IMAGENET_MEAN_1) / IMAGENET_STD_1).reshape(1, -1, 1, 1)).to(DEVICE)\n",
    "\n",
    "\n",
    "def gradient_ascent(config, model, input_tensor, layer_ids_to_use, iteration):\n",
    "    # Step 0: Feed forward pass\n",
    "    out = model(input_tensor)\n",
    "\n",
    "    # Step 1: Grab activations/feature maps of interest\n",
    "    activations = [out[layer_id_to_use] for layer_id_to_use in layer_ids_to_use]\n",
    "\n",
    "    # Step 2: Calculate loss over activations\n",
    "    losses = []\n",
    "    for layer_activation in activations:\n",
    "        # Use torch.norm(torch.flatten(layer_activation), p) with p=2 for L2 loss and p=1 for L1 loss. \n",
    "        # But I'll use the MSE as it works really good, I didn't notice any serious change when going to L1/L2.\n",
    "        # using torch.zeros_like as if we wanted to make activations as small as possible but we'll do gradient ascent\n",
    "        # and that will cause it to actually amplify whatever the network \"sees\" thus yielding the famous DeepDream look\n",
    "        loss_component = torch.nn.MSELoss(reduction='mean')(layer_activation, torch.zeros_like(layer_activation))\n",
    "        losses.append(loss_component)\n",
    "\n",
    "    loss = torch.mean(torch.stack(losses))\n",
    "    loss.backward()\n",
    "\n",
    "    # Step 3: Process image gradients (smoothing + normalization, more an art then a science)\n",
    "    grad = input_tensor.grad.data\n",
    "\n",
    "    # Applies 3 Gaussian kernels and thus \"blurs\" or smoothens the gradients and gives visually more pleasing results\n",
    "    # We'll see the details of this one in the next cell and that's all, you now understand DeepDream!\n",
    "    sigma = ((iteration + 1) / config['num_gradient_ascent_iterations']) * 2.0 + config['smoothing_coefficient']\n",
    "    smooth_grad = CascadeGaussianSmoothing(kernel_size=9, sigma=sigma)(grad)  # \"magic number\" 9 just works well\n",
    "\n",
    "    # Normalize the gradients (make them have mean = 0 and std = 1)\n",
    "    # I didn't notice any big difference normalizing the mean as well - feel free to experiment\n",
    "    g_std = torch.std(smooth_grad)\n",
    "    g_mean = torch.mean(smooth_grad)\n",
    "    smooth_grad = smooth_grad - g_mean\n",
    "    smooth_grad = smooth_grad / g_std\n",
    "\n",
    "    # Step 4: Update image using the calculated gradients (gradient ascent step)\n",
    "    input_tensor.data += config['lr'] * smooth_grad\n",
    "\n",
    "    # Step 5: Clear gradients and clamp the data (otherwise values would explode to +- \"infinity\")\n",
    "    input_tensor.grad.data.zero_()\n",
    "    input_tensor.data = torch.max(torch.min(input_tensor, UPPER_IMAGE_BOUND), LOWER_IMAGE_BOUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-richmond",
   "metadata": {},
   "source": [
    "*Note for uber nerds:* ü§ì <br/>\n",
    "*loss.backward(layer) <- original implementation did it like this and it's equivalent to MSE(reduction='sum')/2. Check out my [playground.py](https://github.com/gordicaleksa/pytorch-deepdream/blob/master/playground.py#L95) for more details*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-orientation",
   "metadata": {},
   "source": [
    "Finally let us see how the **gradient smoothing** via the Gaussian is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atmospheric-consensus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.646222Z",
     "start_time": "2023-12-10T22:52:34.073004Z"
    }
   },
   "outputs": [],
   "source": [
    "class CascadeGaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing separately for each channel (depthwise convolution).\n",
    "\n",
    "    Arguments:\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, sigma):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size, kernel_size]\n",
    "\n",
    "        cascade_coefficients = [0.5, 1.0, 2.0]  # std multipliers, hardcoded to use 3 different Gaussian kernels\n",
    "        sigmas = [[coeff * sigma, coeff * sigma] for coeff in cascade_coefficients]  # isotropic Gaussian\n",
    "\n",
    "        self.pad = int(kernel_size[0] / 2)  # assure we have the same spatial resolution\n",
    "\n",
    "        # The gaussian kernel is the product of the gaussian function of each dimension.\n",
    "        kernels = []\n",
    "        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])\n",
    "        for sigma in sigmas:\n",
    "            kernel = torch.ones_like(meshgrids[0])\n",
    "            for size_1d, std_1d, grid in zip(kernel_size, sigma, meshgrids):\n",
    "                mean = (size_1d - 1) / 2\n",
    "                kernel *= 1 / (std_1d * math.sqrt(2 * math.pi)) * torch.exp(-((grid - mean) / std_1d) ** 2 / 2)\n",
    "            kernels.append(kernel)\n",
    "\n",
    "        gaussian_kernels = []\n",
    "        for kernel in kernels:\n",
    "            # Normalize - make sure sum of values in gaussian kernel equals 1.\n",
    "            kernel = kernel / torch.sum(kernel)\n",
    "            # Reshape to depthwise convolutional weight\n",
    "            kernel = kernel.view(1, 1, *kernel.shape)\n",
    "            kernel = kernel.repeat(3, 1, 1, 1)\n",
    "            kernel = kernel.to(DEVICE)\n",
    "\n",
    "            gaussian_kernels.append(kernel)\n",
    "\n",
    "        self.weight1 = gaussian_kernels[0]\n",
    "        self.weight2 = gaussian_kernels[1]\n",
    "        self.weight3 = gaussian_kernels[2]\n",
    "        self.conv = F.conv2d\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.pad(input, [self.pad, self.pad, self.pad, self.pad], mode='reflect')\n",
    "\n",
    "        # Apply Gaussian kernels depthwise over the input (hence groups equals the number of input channels)\n",
    "        # shape = (1, 3, H, W) -> (1, 3, H, W)\n",
    "        num_in_channels = input.shape[1]\n",
    "        grad1 = self.conv(input, weight=self.weight1, groups=num_in_channels)\n",
    "        grad2 = self.conv(input, weight=self.weight2, groups=num_in_channels)\n",
    "        grad3 = self.conv(input, weight=self.weight3, groups=num_in_channels)\n",
    "\n",
    "        return (grad1 + grad2 + grad3) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-harvest",
   "metadata": {},
   "source": [
    "What it does is it creates three **9x9 Gaussian kernels** and it applies them, **depthwise**, over the input gradients.\n",
    "\n",
    "Kernel 1 is applied to channels 1, 2 and 3 and it preserves the shape i.e. we go from (1, 3, H, W) to (1, 3, H, W). <br/>\n",
    "Similarly for kernels 2 and 3 and then we just combine the output by taking a mean.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-faculty",
   "metadata": {},
   "source": [
    "Phew! That was a mouthful. üéâüéâüéâ\n",
    "\n",
    "Take your time and **be patient**! Especially if you're new to CNNs.\n",
    "\n",
    "I didn't learn all of this in 1 day, it takes time for the knowledge to sink in. You'll get there as well! ‚ù§Ô∏è (if you're not already there üòú)\n",
    "\n",
    "‚úÖ Understand the DeepDream algorithm\n",
    "\n",
    "---\n",
    "\n",
    "Now let's finally define that **config** oracle we've been seeing thrown all around.\n",
    "\n",
    "## Input arguments and run the damn thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "embedded-arabic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.646840Z",
     "start_time": "2023-12-10T22:52:34.078517Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only a small subset is exposed by design to avoid cluttering\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Common params\n",
    "parser.add_argument(\"--input\", type=str, help=\"Input IMAGE or VIDEO name that will be used for dreaming\", default='campanile.jpeg')\n",
    "parser.add_argument(\"--img_width\", type=int, help=\"Resize input image to this width\", default=600)\n",
    "parser.add_argument(\"--layers_to_use\", type=str, nargs='+', help=\"Layer whose activations we should maximize while dreaming\", default=['relu4_3'])\n",
    "parser.add_argument(\"--model_name\", choices=[m.name for m in SupportedModels],\n",
    "                    help=\"Neural network (model) to use for dreaming\", default=SupportedModels.VGG16_EXPERIMENTAL.name)\n",
    "parser.add_argument(\"--pretrained_weights\", choices=[pw.name for pw in SupportedPretrainedWeights],\n",
    "                    help=\"Pretrained weights to use for the above model\", default=SupportedPretrainedWeights.IMAGENET.name)\n",
    "\n",
    "# Main params for experimentation (especially pyramid_size and pyramid_ratio)\n",
    "parser.add_argument(\"--pyramid_size\", type=int, help=\"Number of images in an image pyramid\", default=4)\n",
    "parser.add_argument(\"--pyramid_ratio\", type=float, help=\"Ratio of image sizes in the pyramid\", default=1.8)\n",
    "parser.add_argument(\"--num_gradient_ascent_iterations\", type=int, help=\"Number of gradient ascent iterations\", default=10)\n",
    "parser.add_argument(\"--lr\", type=float, help=\"Learning rate i.e. step size in gradient ascent\", default=0.09)\n",
    "\n",
    "# You usually won't need to change these as often\n",
    "parser.add_argument(\"--should_display\", type=bool, help=\"Display intermediate dreaming results\", default=False)\n",
    "parser.add_argument(\"--spatial_shift_size\", type=int, help='Number of pixels to randomly shift image before grad ascent', default=32)\n",
    "parser.add_argument(\"--smoothing_coefficient\", type=float, help='Directly controls standard deviation for gradient smoothing', default=0.5)\n",
    "parser.add_argument(\"--use_noise\", type=bool, help=\"Use noise as a starting point instead of input image\", default=False)\n",
    "args = parser.parse_args('')  # important to put '' in Jupyter otherwise it will complain\n",
    "\n",
    "# Wrapping configuration into a dictionary\n",
    "config = dict()\n",
    "for arg in vars(args):\n",
    "    config[arg] = getattr(args, arg)\n",
    "config['dump_dir'] = os.path.join(OUT_IMAGES_PATH, f'{config[\"model_name\"]}_{config[\"pretrained_weights\"]}')\n",
    "config['input'] = os.path.basename(config['input'])  # handle absolute and relative paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-airfare",
   "metadata": {},
   "source": [
    "You should be familiar with these at this point.\n",
    "\n",
    "In the **first group of params** we're picking the image we'll be \"dreaming over\", the output image width, which layers/activations should we use to amplify the features, and finally which CNN model and which pretrained weights should we use?\n",
    "\n",
    "The **second group** controls the optimization: the pyramid control parameters, number of optimization steps and the learning rate.\n",
    "\n",
    "You probably won't experiment that much with the **third group of parameters**. The `smoothing coefficient` might hold some promise I didn't experiment that much with it.\n",
    "\n",
    "Finally let's run it, and enjoy the fruits of our labor! üòç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'campanile.jpeg',\n 'img_width': 600,\n 'layers_to_use': ['relu4_3'],\n 'model_name': 'VGG16_EXPERIMENTAL',\n 'pretrained_weights': 'IMAGENET',\n 'pyramid_size': 4,\n 'pyramid_ratio': 1.8,\n 'num_gradient_ascent_iterations': 10,\n 'lr': 0.09,\n 'should_display': False,\n 'spatial_shift_size': 32,\n 'smoothing_coefficient': 0.5,\n 'use_noise': False,\n 'dump_dir': '/Users/cecilerousseau/Documents/BERKELEY/STAT 254 001 - STATS & ML/Final_Project/pytorch-deepdream/data/out-images/VGG16_EXPERIMENTAL_IMAGENET'}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T22:52:34.701652Z",
     "start_time": "2023-12-10T22:52:34.093254Z"
    }
   },
   "id": "4c86664182857f3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = deep_dream_static_image(config)  # yep a single liner\n",
    "config['should_display'] = True\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-10T22:52:34.099Z"
    }
   },
   "id": "63b091812399a03f"
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-mixture",
   "metadata": {},
   "source": [
    "WooOOOooooHhhOOooooo! ‚ù§Ô∏è This is probably one of my favourite algorithms.\n",
    "\n",
    "Who would have known that neural nets had this hidden inside them???\n",
    "\n",
    "---\n",
    "\n",
    "**Congrats!** you went through the hardest part of understanding how DeepDream actually works! üòç\n",
    "\n",
    "Having said that I have to confess. \n",
    "\n",
    "If your only goal is to create beautiful artistic imagery, you don't necessarily need to understand how it works under the hood! Don't be mad at me. ‚ù§Ô∏è \n",
    "\n",
    "That understanding can still help you make smarter decisions as you'll soon see.\n",
    "\n",
    "The next section will give you the power to create awesome images. For free. On your computer. And very fast! (at least on my machine - RTX 2080 üòÇ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-judges",
   "metadata": {},
   "source": [
    "# Part 2: Experimenting with static DeepDream images\n",
    "\n",
    "Let me now show you how you can tweak the DeepDream settings. ‚öôÔ∏è\n",
    "\n",
    "Once you get the feeling for how it works, you'll be much more capable to produce the imagery that you want! üé®\n",
    "\n",
    "---\n",
    "\n",
    "Setting 1: `use_noise` üòç\n",
    "\n",
    "Instead of using a specified image like `figures.jpg`, which I've been using as the running example, you can initialize the algorithm with pure noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "91309718ee09a725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layers_to_plot = ['relu3_1', 'relu3_2', 'relu3_3', 'relu4_1', 'relu4_2', 'relu4_3', 'relu5_1', 'relu5_2', 'relu5_3', 'mp5']\n",
    "\n",
    "for layer_to_plot in layers_to_plot:\n",
    "    \n",
    "    config['layers_to_use'] = np.array([ layer_to_plot ])\n",
    "    config['use_noise'] = True  # variable under test\n",
    "    \n",
    "    img = deep_dream_static_image(config)\n",
    "    dump_path = save_and_maybe_display_image(config, img)\n",
    "    print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "    \n",
    "    config['use_noise'] = False  # keep the config consistent from cell to cell"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "996ed2d4895f5663"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32d23a9ea4397c64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "config['layers_to_use'] = np.array(['relu3_1'])\n",
    "config['img_width'] = 400\n",
    "\n",
    "config['use_noise'] = True  # variable under test\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config['img_width'] = 600\n",
    "config['use_noise'] = False  # keep the config consistent from cell to cell"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8374cb212ac79bca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config['layers_to_use'] = np.array(['relu3_2'])\n",
    "config['use_noise'] = True  # variable under test\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config['use_noise'] = False  # keep the config consistent from cell to cell"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c5b522b51459fb7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-portable",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "config['layers_to_use'] = np.array(['relu3_3'])\n",
    "config['use_noise'] = True  # variable under test\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config['use_noise'] = False  # keep the config consistent from cell to cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config['layers_to_use']= np.array(['relu4_3'])\n",
    "config['use_noise'] = True  # variable under test\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config['use_noise'] = False  # keep the config consistent from cell to cell\n",
    "config['layers_to_use']= np.array(['relu4_3'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3a22393b034a3c1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config['layers_to_use']= np.array(['relu5_3'])\n",
    "config['use_noise'] = True  # variable under test\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config['use_noise'] = False  # keep the config consistent from cell to cell\n",
    "config['layers_to_use']= np.array(['relu4_3'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1419acbe105776b0"
  },
  {
   "cell_type": "markdown",
   "id": "optical-sequence",
   "metadata": {},
   "source": [
    "No trace of the `figures.jpg` image! The results are much less predictable. TBH, I rarely used it, but in combination with other arguments it may give you some surprising results!\n",
    "\n",
    "---\n",
    "\n",
    "Setting 2: `layers_to_use` üòç\n",
    "\n",
    "So far we've been using VGG16's `relu4_3` layer. \n",
    "\n",
    "Depending on the layer you choose you'll amplify **lower** (e.g. edges/patterns) or **higher** (e.g. animal eyes/mouths) image level features and everything in betweeen - it's a spectrum. You can also combine multiple layers together like `['relu4_3', 'relu5_1']` but the combinatorics quickly explodes, so I'll just show you a single layer examples here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-peter",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "exposed_layers = Vgg16Experimental(pretrained_weights=SupportedPretrainedWeights.IMAGENET.name).layer_names\n",
    "exposed_layers = exposed_layers[:-3]  # truncate the ugly ones (need to keep this Jupyter notebook's filesize reasonable!)\n",
    "print(f'Exposed layers = {exposed_layers}')\n",
    "\n",
    "for layer in exposed_layers:\n",
    "    config['layers_to_use'] = [layer]  # You can also use multiple layers here\n",
    "\n",
    "    img = deep_dream_static_image(config_cecile)\n",
    "    dump_path = save_and_maybe_display_image(config_cecile, img)\n",
    "    print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config_cecile['layers_to_use'] = ['relu4_3']  # keep the config consistent from cell to cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-deadline",
   "metadata": {},
   "source": [
    "You can see that as we're progressing towards deeper layers the features are becoming more complex i.e. \"higher level\".\n",
    "\n",
    "---\n",
    "\n",
    "Setting 3: `pyramid_size` and `pyramid_ratio` üòç\n",
    "\n",
    "We already learned what those are in the Image pyramid section.\n",
    "\n",
    "Nice, well, let's experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-overall",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "pyramid_sizes = [1, 3, 5]  # super arbitrary, go beserk\n",
    "config_cecile['pyramid_ratio'] = 1.4  # feel free to play with this one as well\n",
    "\n",
    "for pyramid_size in pyramid_sizes:\n",
    "    config_cecile['pyramid_size'] = pyramid_size\n",
    "\n",
    "    img = deep_dream_static_image(config_cecile)\n",
    "    dump_path = save_and_maybe_display_image(config_cecile, img)\n",
    "    print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config_cecile['pyramid_size'] = 4  # keep the config consistent from cell to cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-orientation",
   "metadata": {},
   "source": [
    "\n",
    "You can nicely see that the higher the pyramid the more psychodelic the images become. \n",
    "\n",
    "It's simple - they got more processing/dreaming.\n",
    "\n",
    "---\n",
    "\n",
    "Setting 4: `num_gradient_ascent_iterations` and `lr` üòç\n",
    "\n",
    "These 2 have, in a way, a similar effect. The higher the numbers the more amplification you'll get.\n",
    "\n",
    "* `lr` (learning rate) - has a narrow range in which it gives really nice results. Leave it too small and the images remain unchanged, set it too *high* and imagery becomes overly psychodelic (you see what I did there?). I found `0.09` to be a nice number.\n",
    "* `num_gradient_ascent_iterations` - as soon as you get to 3+ iterations it starts to give nice results.\n",
    "\n",
    "I don't want to overengineer with explanations here. If you understood the algorithm you'll know exactly what these do! ü§ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-fleece",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_gradient_ascent_iterations = [2, 5, 20]\n",
    "config_cecile['lr'] = 0.09  # feel free to play with this one as well\n",
    "\n",
    "for num_iter in num_gradient_ascent_iterations:\n",
    "    config_cecile['num_gradient_ascent_iterations'] = num_iter\n",
    "\n",
    "    img = deep_dream_static_image(config_cecile)\n",
    "    dump_path = save_and_maybe_display_image(config_cecile, img)\n",
    "    print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')\n",
    "\n",
    "config_cecile['num_gradient_ascent_iterations'] = 10  # keep the config consistent from cell to cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-bread",
   "metadata": {},
   "source": [
    "Yep as you can see the more iterations the stronger the colors and details. But at one point it breaks and starts giving uglier results, IMHO. This is not exactly science you know! [De gustibus non est disputandum.](https://en.wikipedia.org/wiki/De_gustibus_non_est_disputandum) ‚ù§Ô∏è\n",
    "\n",
    "Ok, we're done with experiments. There are a couple more arguments hidden in the code but these were the main ones.\n",
    "\n",
    "`model_name` and `pretrained_weights` have a high impact as well, but I decided to keep this notebook shorter than [Anna Karenina](https://en.wikipedia.org/wiki/Anna_Karenina). <br/>\n",
    "Because I love you, not because I'm lazy.\n",
    "\n",
    "---\n",
    "\n",
    "Let me now show you one more neat thing. **We can reconstruct all of the images in the README** using only their filenames!\n",
    "\n",
    "Reproducibility is super important in general.\n",
    "\n",
    "*Note: there a couple of 'gotchas'. Some ambiguities do exist. For example the `layer3` in ResNet50 (which you'll see in the next section) could mean a couple of things like `layer30`, `layer31`, `layer32`, `layer34` or `layer35`. But it's super easy to figure out which one was used - just try all of them! It is possible to avoid this altogether (by increasing the namedtuple variable), it's just that the code would become somewhat clunkier.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-fever",
   "metadata": {},
   "source": [
    "# Reconstructing README images üìú\n",
    "\n",
    "Let's start with the lion!\n",
    "\n",
    "This is the image name:\n",
    "`lion_width_600_model_VGG16_EXPERIMENTAL_IMAGENET_relu3_3_pyrsize_4_pyrratio_1.8_iter_10_lr_0.09_shift_32_resized400.jpg`\n",
    "\n",
    "By parsing it we can create the `config` in the following cell.\n",
    "\n",
    "*Note: `smoothing_coefficient` coefficient wasn't included in the older version of the code but it was kept at 0.5 (and that's still the default value)*\n",
    "\n",
    "*Note2: `resized400` means that I've manually resized the result to width=400 to keep the README loading responsive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-memorial",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "config['input'] = 'ours.jpeg'\n",
    "config['img_width'] = 600\n",
    "config['model_name'] = SupportedModels.VGG16_EXPERIMENTAL.name\n",
    "config['pretrained_weights'] = SupportedPretrainedWeights.IMAGENET.name\n",
    "config['layers_to_use'] = ['relu3_3']\n",
    "config['pyramid_size'] = 4\n",
    "config['pyramid_ratio'] = 1.8\n",
    "config['num_gradient_ascent_iterations'] = 10\n",
    "config['lr'] = 0.09\n",
    "config['spatial_shift_size'] = 32\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-yugoslavia",
   "metadata": {},
   "source": [
    "Wohooo! We've reconstructed the image using the info encoded in the filename! Check out the [README](https://github.com/gordicaleksa/pytorch-deepdream) if you don't believe me.\n",
    "\n",
    "As you can see it's possible to entirely reconstruct the image using only the filename! ‚ù§Ô∏è\n",
    "\n",
    "There is obviously some stochasticity in the algorithm so it's hard to get the exact same result but this is as close as it gets!\n",
    "\n",
    "---\n",
    "\n",
    "Let's do 1 more (since it was created using `VGG16_EXPERIMENTAL` model):\n",
    "\n",
    "image name: <br/> `green_bridge_width_600_model_VGG16_EXPERIMENTAL_IMAGENET_relu4_3_pyrsize_4_pyrratio_1.8_iter_10_lr_0.09_shift_32_resized.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-mainland",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "config['input'] = 'green_bridge.jpg'\n",
    "config['img_width'] = 600\n",
    "config['model_name'] = SupportedModels.VGG16_EXPERIMENTAL.name\n",
    "config['pretrained_weights'] = SupportedPretrainedWeights.IMAGENET.name\n",
    "config['layers_to_use'] = ['relu4_3']\n",
    "config['pyramid_size'] = 4\n",
    "config['pyramid_ratio'] = 1.8\n",
    "config['num_gradient_ascent_iterations'] = 10\n",
    "config['lr'] = 0.09\n",
    "config['spatial_shift_size'] = 32\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-diary",
   "metadata": {},
   "source": [
    "Awesome!!! As a bonus let's reconstruct The AI Epiphany's logo. üòç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-reply",
   "metadata": {},
   "source": [
    "# Reconstructing The AI Epiphany logo\n",
    "\n",
    "For that we'll need to define ResNet50 as well, so let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-princeton",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from torch.hub import download_url_to_file\n",
    "\n",
    "class ResNet50(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_weights, requires_grad=False, show_progress=False):\n",
    "        super().__init__()\n",
    "        if pretrained_weights == SupportedPretrainedWeights.IMAGENET.name:\n",
    "            resnet50 = models.resnet50(pretrained=True, progress=show_progress).eval()\n",
    "            \n",
    "        elif pretrained_weights == SupportedPretrainedWeights.PLACES_365.name:\n",
    "            resnet50 = models.resnet50(pretrained=False, progress=show_progress).eval()\n",
    "\n",
    "            binary_name = 'resnet50_places365.pth.tar'\n",
    "            resnet50_places365_binary_path = os.path.join(BINARIES_PATH, binary_name)\n",
    "\n",
    "            if os.path.exists(resnet50_places365_binary_path):\n",
    "                state_dict = torch.load(resnet50_places365_binary_path, map_location=torch.device('cpu'))['state_dict']\n",
    "            else:\n",
    "                binary_url = r'http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar'\n",
    "                print(f'Downloading {binary_name} from {binary_url} it may take some time.')\n",
    "                download_url_to_file(binary_url, resnet50_places365_binary_path)\n",
    "                print('Done downloading.')\n",
    "                state_dict = torch.load(resnet50_places365_binary_path)['state_dict']\n",
    "\n",
    "            new_state_dict = {}  # modify key names and make it compatible with current PyTorch model naming scheme\n",
    "            for old_key in state_dict.keys():\n",
    "                new_key = old_key[7:]\n",
    "                new_state_dict[new_key] = state_dict[old_key]\n",
    "\n",
    "            resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 365)\n",
    "            resnet50.load_state_dict(new_state_dict, strict=True)\n",
    "        else:\n",
    "            raise Exception(f'Pretrained weights {pretrained_weights} not yet supported for {self.__class__.__name__} model.')\n",
    "\n",
    "        self.layer_names = ['layer1', 'layer2', 'layer3', 'layer4']\n",
    "\n",
    "        self.conv1 = resnet50.conv1\n",
    "        self.bn1 = resnet50.bn1\n",
    "        self.relu = resnet50.relu\n",
    "        self.maxpool = resnet50.maxpool\n",
    "\n",
    "        # 3\n",
    "        self.layer10 = resnet50.layer1[0]\n",
    "        self.layer11 = resnet50.layer1[1]\n",
    "        self.layer12 = resnet50.layer1[2]\n",
    "\n",
    "        # 4\n",
    "        self.layer20 = resnet50.layer2[0]\n",
    "        self.layer21 = resnet50.layer2[1]\n",
    "        self.layer22 = resnet50.layer2[2]\n",
    "        self.layer23 = resnet50.layer2[3]\n",
    "\n",
    "        # 6\n",
    "        self.layer30 = resnet50.layer3[0]\n",
    "        self.layer31 = resnet50.layer3[1]\n",
    "        self.layer32 = resnet50.layer3[2]\n",
    "        self.layer33 = resnet50.layer3[3]\n",
    "        self.layer34 = resnet50.layer3[4]\n",
    "        self.layer35 = resnet50.layer3[5]\n",
    "\n",
    "        # 3\n",
    "        self.layer40 = resnet50.layer4[0]\n",
    "        self.layer41 = resnet50.layer4[1]\n",
    "        # self.layer42 = resnet50.layer4[2]\n",
    "\n",
    "        # Go even deeper into ResNet's BottleNeck module for layer 42\n",
    "        self.layer42_conv1 = resnet50.layer4[2].conv1\n",
    "        self.layer42_bn1 = resnet50.layer4[2].bn1\n",
    "        self.layer42_conv2 = resnet50.layer4[2].conv2\n",
    "        self.layer42_bn2 = resnet50.layer4[2].bn2\n",
    "        self.layer42_conv3 = resnet50.layer4[2].conv3\n",
    "        self.layer42_bn3 = resnet50.layer4[2].bn3\n",
    "        self.layer42_relu = resnet50.layer4[2].relu\n",
    "\n",
    "        # Set these to False so that PyTorch won't be including them in its autograd engine - eating up precious memory\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Feel free to experiment with different layers\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer10(x)\n",
    "        layer10 = x\n",
    "        x = self.layer11(x)\n",
    "        layer11 = x\n",
    "        x = self.layer12(x)\n",
    "        layer12 = x\n",
    "        x = self.layer20(x)\n",
    "        layer20 = x\n",
    "        x = self.layer21(x)\n",
    "        layer21 = x\n",
    "        x = self.layer22(x)\n",
    "        layer22 = x\n",
    "        x = self.layer23(x)\n",
    "        layer23 = x\n",
    "        x = self.layer30(x)\n",
    "        layer30 = x\n",
    "        x = self.layer31(x)\n",
    "        layer31 = x\n",
    "        x = self.layer32(x)\n",
    "        layer32 = x\n",
    "        x = self.layer33(x)\n",
    "        layer33 = x\n",
    "        x = self.layer34(x)\n",
    "        layer34 = x\n",
    "        x = self.layer35(x)\n",
    "        layer35 = x\n",
    "        x = self.layer40(x)\n",
    "        layer40 = x\n",
    "        x = self.layer41(x)\n",
    "        layer41 = x\n",
    "\n",
    "        layer42_identity = layer41\n",
    "        x = self.layer42_conv1(x)\n",
    "        layer420 = x\n",
    "        x = self.layer42_bn1(x)\n",
    "        layer421 = x\n",
    "        x = self.layer42_relu(x)\n",
    "        layer422 = x\n",
    "        x = self.layer42_conv2(x)\n",
    "        layer423 = x\n",
    "        x = self.layer42_bn2(x)\n",
    "        layer424 = x\n",
    "        x = self.layer42_relu(x)\n",
    "        layer425 = x\n",
    "        x = self.layer42_conv3(x)\n",
    "        layer426 = x\n",
    "        x = self.layer42_bn3(x)\n",
    "        layer427 = x\n",
    "        x += layer42_identity\n",
    "        layer428 = x\n",
    "        x = self.relu(x)\n",
    "        layer429 = x\n",
    "\n",
    "        # Feel free to experiment with different layers, layer35 is my favourite\n",
    "        net_outputs = namedtuple(\"ResNet50Outputs\", self.layer_names)\n",
    "        # You can see the potential ambiguity arising here if we later want to reconstruct images purely from the filename\n",
    "        out = net_outputs(layer10, layer23, layer34, layer40)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-forge",
   "metadata": {},
   "source": [
    "So, the moment has come. Let's reconstruct that glorious logo.\n",
    "\n",
    "image name: <br/>\n",
    "`figures_width_960_model_RESNET50_PLACES_365_layer3_pyrsize_4_pyrratio_1.8_iter_10_lr_0.09_shift_40.jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-diesel",
   "metadata": {
    "scrolled": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "config['input'] = 'campanile.jpeg'\n",
    "config['img_width'] = 960\n",
    "config['model_name'] = SupportedModels.RESNET50.name\n",
    "config['pretrained_weights'] = SupportedPretrainedWeights.PLACES_365.name\n",
    "config['layers_to_use'] = ['layer3']  # layer34 was used\n",
    "config['pyramid_size'] = 4\n",
    "config['pyramid_ratio'] = 1.8\n",
    "config['num_gradient_ascent_iterations'] = 10\n",
    "config['lr'] = 0.09\n",
    "config['spatial_shift_size'] = 40\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config_cecile"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "269e7736beff5f60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config['input'] = 'wanderise.jpg'\n",
    "config['img_width'] = 960\n",
    "config['model_name'] = SupportedModels.VGG16_EXPERIMENTAL.name\n",
    "config['pretrained_weights'] = SupportedPretrainedWeights.IMAGENET.name\n",
    "config['layers_to_use'] = ['relu4_3']  # layer34 was used\n",
    "config['pyramid_size'] = 4\n",
    "config['pyramid_ratio'] = 1.8\n",
    "config['num_gradient_ascent_iterations'] = 10\n",
    "config['lr'] = 0.09\n",
    "config['spatial_shift_size'] = 40\n",
    "\n",
    "img = deep_dream_static_image(config)\n",
    "dump_path = save_and_maybe_display_image(config, img)\n",
    "print(f'Saved DeepDream static image to: {os.path.relpath(dump_path)}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d5c36348fe3cbf1a"
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-breed",
   "metadata": {},
   "source": [
    "WohooOooOoo! üéâüéâüéâ\n",
    "\n",
    "Again, there is obviously some stochasticity in the algorithm so it's hard to get the exact same result but this is as close as it gets!\n",
    "\n",
    "---\n",
    "\n",
    "**Things I haven't covered in this Jupyter notebook:**\n",
    "* DeepDream video creation using blending\n",
    "* Ouroboros video (feeding the output from DeepDream static into the input and applying some geometric transformations)\n",
    "\n",
    "And a couple of functions that I've defined in the `playground.py` file, like creating GIFs, but all of those are basically wrappers around the functionality you just saw! Having said that, the code in this repository is at your disposal, so either open up your favourite **IDE** or you can also use the **command line!**.\n",
    "\n",
    "And last but not least!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-stranger",
   "metadata": {},
   "source": [
    "# Connect with me\n",
    "\n",
    "I share lots of useful (I hope so at least!) content on LinkedIn, Twitter, YouTube and Medium. <br/>\n",
    "So feel free to connect with me there:\n",
    "1. My [LinkedIn](https://www.linkedin.com/in/aleksagordic) and [Twitter](https://twitter.com/gordic_aleksa) profiles\n",
    "2. My YouTube channel - [The AI Epiphany](https://www.youtube.com/c/TheAiEpiphany)\n",
    "3. My [Medium](https://gordicaleksa.medium.com/) profile\n",
    "\n",
    "Also do drop me a message if you found this useful or if you think I could've done something better! <br/>\n",
    "I always like getting some feedback on the work I do.\n",
    "\n",
    "If you notice some bugs/errors feel free to **open up an issue** or even **submit a pull request**.\n",
    "\n",
    "# Additional resources\n",
    "\n",
    "If you're interested in learning more about the DeepDream algorithm aside from this Jupyter notebook and the [official blog](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) you may find [this video on DeepDream theory](https://www.youtube.com/watch?v=6rVrh5gnpwk) useful!\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=6rVrh5gnpwk\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/6rVrh5gnpwk/0.jpg\" \n",
    "alt=\"DeepDream theory explained\" width=\"480\" align=\"left\" height=\"360\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "68130fb9fcda3708"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f75b8278d18257a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1b09e4ccfdf919b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
